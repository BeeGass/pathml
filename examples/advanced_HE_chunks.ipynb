{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced H&E Workflow\n",
    "\n",
    "In the basic H&E workflow, each whole-slide image was loaded into memory at full-resolution, and then preprocessing was applied to the entire image at once. However, in some cases, this may not be a good option. For example, you may have images that are simply too big to load in their entirety at full-resolution. Also, some steps may be applied at lower-resolution levels, thus increasing efficiency. \n",
    "\n",
    "In this notebook, we will show how to use `PathML` to develop a custom preprocessing pipeline that takes these factors into account. We will still be performing the same task as in the basic example notebook, i.e. detecting regions of tissue and extracting tiles. However, in this pipeline we will perform tissue detection using a low-resolution image of the slide, and extract tiles by processing the original image in chunks.\n",
    "\n",
    "For each slide:\n",
    "\n",
    "1. Detect regions of tissue, using low-resolution image of whole slide\n",
    "2. load full-resolution slide in chunks\n",
    "3. divide each chunk into 224px tiles\n",
    "\n",
    "The ability to define our own custom TileExtractor class allows us to create this pipeline.\n",
    "\n",
    "1. `SlideLoader` loads low-resolution image into `SlideData.image`\n",
    "2. `SlidePreprocessor` performs tissue detection on the low-res image, and puts the mask into `SlideData.mask`\n",
    "3. `TileExtractor`:\n",
    "    - dimensions of full-resolution image are divided into 5000px chunks\n",
    "    - for each chunk:\n",
    "        - use `SlideData.wsi.read_region()` to read full-resolution image for chunk\n",
    "        - get corresponding mask region from tissue detection\n",
    "        - upsample the mask to match full-resolution image\n",
    "        - divide into tiles\n",
    "4. `TilePreprocessor`\n",
    "    - filters out any whitespace tiles\n",
    "    - applies Macenko stain normalization\n",
    "    - writes tiles to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathml.preprocessing.tiling import extract_tiles_with_mask\n",
    "from pathml.preprocessing.utils import pil_to_rgb, upsample_array, plot_mask, label_whitespace_HE\n",
    "from pathml.preprocessing.pipeline import Pipeline\n",
    "from pathml.preprocessing.base_preprocessor import (BaseSlideLoader,\n",
    "                                                    BaseSlidePreprocessor,\n",
    "                                                    BaseTileExtractor,\n",
    "                                                    BaseTilePreprocessor)\n",
    "from pathml.preprocessing.transforms_HandE import TissueDetectionHE\n",
    "from pathml.preprocessing.wsi import HESlide\n",
    "from pathml.preprocessing.transforms import ForegroundDetection\n",
    "from pathml.preprocessing.stains import StainNormalizationHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "class MySlideLoader(BaseSlideLoader):\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "    \n",
    "    def apply(self, path):\n",
    "        data = HESlide(path).load_data(level=self.level)\n",
    "        # add the level as an attribute to the SlideData object so we can access it later\n",
    "        data.level = self.level\n",
    "        return data\n",
    "\n",
    "# step 2\n",
    "class MySlidePreprocessor(BaseSlidePreprocessor):\n",
    "    \"\"\"slide-level preprocessor which detects regions of tissue\"\"\"\n",
    "    def apply(self, data):\n",
    "        # using downsampled image, so need to lower min_region_size for tissue detection\n",
    "        tissue_detector = TissueDetectionHE(\n",
    "            foreground_detection = ForegroundDetection(min_region_size=1000, max_hole_size=1000)\n",
    "        )\n",
    "        tissue_mask = tissue_detector.apply(data.image)\n",
    "        data.mask = tissue_mask\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the trickier part, which is tile extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "class MyTileExtractor(BaseTileExtractor):\n",
    "    \n",
    "    def __init__(self, tile_size=224, chunk_size_low_res = 1000):\n",
    "        self.tile_size = 224\n",
    "        # size of each chunk, at low-resolution\n",
    "        self.chunk_size_low_res = chunk_size_low_res\n",
    "    \n",
    "    def apply(self, data):\n",
    "        \"\"\"\n",
    "        Use the downsampled data.mask to get full-resolution tiles. \n",
    "        Process full-resolution image in chunks.\n",
    "        \"\"\"\n",
    "        # get scale for upscaling mask to full-res\n",
    "        scale = data.wsi.slide.level_downsamples[data.level]\n",
    "        scale = int(scale)\n",
    "        # size of each chunk, at low-resolution\n",
    "        chunk_size_low_res = self.chunk_size_low_res\n",
    "        # size of each chunk, at full-resolution\n",
    "        chunk_size = chunk_size_low_res * scale\n",
    "        # how many chunks in each full_res dim\n",
    "        # note that openslide uses (width, height) format\n",
    "        full_res_j, full_res_i = data.wsi.slide.level_dimensions[0]\n",
    "        # loop thru chunks\n",
    "        n_chunk_i = full_res_i // chunk_size\n",
    "        n_chunk_j = full_res_j // chunk_size\n",
    "        \n",
    "        for ix_i in range(n_chunk_i):\n",
    "            for ix_j in range(n_chunk_j):\n",
    "                # get mask\n",
    "                mask = data.mask[ix_i*chunk_size_low_res:(ix_i + 1)*chunk_size_low_res, \n",
    "                                 ix_j*chunk_size_low_res:(ix_j + 1)*chunk_size_low_res]\n",
    "                \n",
    "                if mask.mean() == 0.0:\n",
    "                    # empty chunk, no need to continue processing\n",
    "                    continue\n",
    "                # upscale mask to match full-res image\n",
    "                mask_upsampled = upsample_array(mask, scale)\n",
    "                # get full-res image\n",
    "                region = data.wsi.slide.read_region(\n",
    "                    location = (ix_j*chunk_size, ix_i*chunk_size),\n",
    "                    level = 0, size = (chunk_size, chunk_size)\n",
    "                )\n",
    "                region_rgb = pil_to_rgb(region)\n",
    "                \n",
    "                # divide into tiles\n",
    "                good_tiles = extract_tiles_with_mask(\n",
    "                    im = region_rgb, \n",
    "                    tile_size = self.tile_size,\n",
    "                    mask = mask_upsampled\n",
    "                )\n",
    "                \n",
    "                for tile in good_tiles:\n",
    "                    # adjust i and j coordinates for each tile to account for the chunk offset\n",
    "                    tile.i += ix_i*chunk_size\n",
    "                    tile.j += ix_j*chunk_size\n",
    "                \n",
    "                # add extracted tiles to data.tiles\n",
    "                data.tiles = good_tiles if data.tiles is None else data.tiles + good_tiles\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the exact same code for the tile-level preprocessor as we used in the Basic H&E example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "class MyTilePreprocessor(BaseTilePreprocessor):\n",
    "    \"\"\"\n",
    "    Simple tile preprocessor which applies color normalizations, \n",
    "    filters out whitespace tiles, and writes tiles to disk\n",
    "    \"\"\"\n",
    "    def apply(self, data):\n",
    "        normalizer = StainNormalizationHE(stain_estimation_method='macenko')\n",
    "        # save the processed tiles to a new directory in same location as original wsi\n",
    "        out_dir = os.path.join(\n",
    "            os.path.dirname(data.wsi.path), \n",
    "            os.path.splitext(os.path.basename(data.wsi.path))[0] + \"_tiled\"\n",
    "        )\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        # extra step to filter out whitespace tiles\n",
    "        data.tiles[:] = [tile for tile in data.tiles if not label_whitespace_HE(tile.array)]\n",
    "        # now loop through tiles, normalize the color, and save to disk\n",
    "        for tile in data.tiles:            \n",
    "            tile.array = normalizer.apply(tile.array)\n",
    "            tile.save(out_dir = out_dir, filename = f\"{data.wsi.name}_{tile.i}_{tile.j}.jpeg\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we string together everything into a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose into pipeline\n",
    "my_pipeline = Pipeline(\n",
    "    slide_loader = MySlideLoader(level = 2),\n",
    "    slide_preprocessor = MySlidePreprocessor(),\n",
    "    tile_extractor = MyTileExtractor(tile_size=224, chunk_size_low_res = 224*4),\n",
    "    tile_preprocessor = MyTilePreprocessor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running pipeline\n",
    "\n",
    "Now, we are ready to try out the pipeline\n",
    "\n",
    "**OpenSlide Data**  \n",
    "This example notebook uses publicly available images from OpenSlide. Download them [here](http://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/) if you want to run this notebook locally, or change the filepaths to any whole-slide images that you have locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image_path = \"../data/CMU-2.svs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 1s, sys: 46.8 s, total: 14min 48s\n",
      "Wall time: 3min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlideData(wsi=HESlide(path=../data/CMU-2.svs, name=CMU-2), image shape: (1903, 4875, 3), mask shape: (1903, 4875), number of tiles: 10786)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "my_pipeline.run(example_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the run-time for this pipeline to that in the Basic H&E example notebook (note that both pipelines were run on the same machine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathml-testing",
   "language": "python",
   "name": "pathml-testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
